{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nltk_cx.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "GYxIxdLOiJJJ",
        "ibpthTCGjNMv"
      ],
      "authorship_tag": "ABX9TyMymh3EhgD9qDh2n1F7RMCE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HARSHIT097/NATURAL_LANGUAGE_PROCESSING/blob/main/nltk_cx.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section-1"
      ],
      "metadata": {
        "id": "GYxIxdLOiJJJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8OreQUqnWck"
      },
      "source": [
        "## intr\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjsvWn6-nigK",
        "outputId": "eebaeacf-6c04-4dbc-dff3-c5fdbf0da8ba"
      },
      "source": [
        "#intr\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "text = \"Python is an interpreted high-level programming language for general-purpose programming. Created by Guido van Rossum and first released in 1991.\"\n",
        "sentences = nltk.sent_tokenize(text)\n",
        "len(sentences)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wj7wQ9jEnjDb",
        "outputId": "a4aa7df3-1fe0-428b-c09d-642cfe27f6b4"
      },
      "source": [
        "words = nltk.word_tokenize(text)\n",
        "#len(words)\n",
        "\n",
        "words[:5]"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Python', 'is', 'an', 'interpreted', 'high-level']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQVrZpXJnqCq",
        "outputId": "2f44f247-95d2-4edc-9b12-be3b882867a9"
      },
      "source": [
        "wordfreq = nltk.FreqDist(words)\n",
        "wordfreq.most_common(2)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('programming', 2), ('.', 2)]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpNfF5clnumA"
      },
      "source": [
        ""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrT-mOmnl_Sx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01177917-ec16-4b9f-a234-0cfc77e60e7c"
      },
      "source": [
        "from nltk.corpus import genesis\n",
        "nltk.download('genesis')\n",
        "nltk.download('stopwords')\n",
        "tokens = genesis.words('english-kjv.txt')\n",
        "gen_text = nltk.Text(tokens)\n",
        "gen_text.collocations()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/genesis.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "said unto; pray thee; thou shalt; thou hast; thy seed; years old;\n",
            "spake unto; thou art; LORD God; every living; God hath; begat sons;\n",
            "seven years; shalt thou; little ones; living creature; creeping thing;\n",
            "savoury meat; thirty years; every beast\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iD8lASbKl4EA",
        "outputId": "0942919f-3697-4aca-d757-a0b796acaaed"
      },
      "source": [
        "from nltk.corpus import genesis\n",
        "tokens = genesis.words('english-kjv.txt')\n",
        "gen_text = nltk.Text(tokens)\n",
        "gen_text.collocations()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "said unto; pray thee; thou shalt; thou hast; thy seed; years old;\n",
            "spake unto; thou art; LORD God; every living; God hath; begat sons;\n",
            "seven years; shalt thou; little ones; living creature; creeping thing;\n",
            "savoury meat; thirty years; every beast\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHZgVhf4fo_B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5877c0f-38be-43f7-9ef7-8e977cfa67dc"
      },
      "source": [
        "import nltk\n",
        "nltk.download('book')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading collection 'book'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection book\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDm2EEvPpSXA"
      },
      "source": [
        "##simple operations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sj-VpLFZpVFF",
        "outputId": "b9fa30cd-11fe-41d3-b3a3-6f6d9a8f83af"
      },
      "source": [
        "from nltk.book import *\n",
        "type(text1)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Introductory Examples for the NLTK Book ***\n",
            "Loading text1, ..., text9 and sent1, ..., sent9\n",
            "Type the name of the text or sentence to view it.\n",
            "Type: 'texts()' or 'sents()' to list the materials.\n",
            "text1: Moby Dick by Herman Melville 1851\n",
            "text2: Sense and Sensibility by Jane Austen 1811\n",
            "text3: The Book of Genesis\n",
            "text4: Inaugural Address Corpus\n",
            "text5: Chat Corpus\n",
            "text6: Monty Python and the Holy Grail\n",
            "text7: Wall Street Journal\n",
            "text8: Personals Corpus\n",
            "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "nltk.text.Text"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-o_3lAVIpdlE",
        "outputId": "d1a1c6f7-8c38-4692-e229-17946f7f9a82"
      },
      "source": [
        "n_words = len(text1)\n",
        "n_words"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "260819"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBfW1fhrpiWa",
        "outputId": "02c263ae-e81f-4723-83d3-19f8138878e7"
      },
      "source": [
        "n_unique_words = len(set(text1))\n",
        "n_unique_words"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19317"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkUiNPQJpiTw",
        "outputId": "2d4a5b5f-9c78-44f1-a2f0-9668c50789f4"
      },
      "source": [
        "text1_lcw = [ word.lower() for word in set(text1) ]\n",
        "n_unique_words_lc = len(set(text1_lcw))\n",
        "n_unique_words_lc"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17231"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Esu85BAJpiQa",
        "outputId": "e7116528-93c2-4f53-85cf-6dd86adce15f"
      },
      "source": [
        "word_coverage1 = n_words / n_unique_words\n",
        "word_coverage1"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13.502044830977896"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AI5qiAwJpiNl",
        "outputId": "13306179-e2ce-42ab-cb42-b112b7327ac4"
      },
      "source": [
        "word_coverage2 = n_words / n_unique_words_lc\n",
        "word_coverage2"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15.136614241773549"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uskWE0f6piKw",
        "outputId": "4c1a2e0f-ca1d-40da-e987-540e59078edf"
      },
      "source": [
        "big_words = [word for word in set(text1) if len(word) > 17 ]\n",
        "big_words"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['uninterpenetratingly', 'characteristically']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jfjtb81npiIC",
        "outputId": "3fb4b8de-836a-46be-f9d8-241fba74b90e"
      },
      "source": [
        "sun_words = [word for word in set(text1) if word.startswith('Sun') ]\n",
        "sun_words"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Sunday', 'Sunset', 'Sunda']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzRpFwtkp5x5",
        "outputId": "5ba620a3-ee6a-48a1-e7b9-3ae7f6f81c22"
      },
      "source": [
        "text1_freq = nltk.FreqDist(text1)\n",
        "text1_freq['Sunday']"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NriW7U2qp5uY",
        "outputId": "2003e952-303f-48aa-c381-8a9af7a42716"
      },
      "source": [
        "top3_text1 = text1_freq.most_common(3)\n",
        "top3_text1"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(',', 18713), ('the', 13721), ('.', 6862)]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6b8QCZ0p5rl",
        "outputId": "9329013d-a473-4609-998b-6666c157dd6c"
      },
      "source": [
        "large_uncommon_words = [word for word in text1 if word.isalpha() and len(word) > 7 ]\n",
        "text1_uncommon_freq = nltk.FreqDist(large_uncommon_words)\n",
        "text1_uncommon_freq.most_common(3)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Queequeg', 252), ('Starbuck', 196), ('something', 119)]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_3rbTL-p5oh"
      },
      "source": [
        ""
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UNhl8dQp5lc"
      },
      "source": [
        ""
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eTqBSJDolL1"
      },
      "source": [
        "##textcorpora"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qohvyld9oxtB"
      },
      "source": [
        "Two popular Text Corpora available from nltk, which you will be using in this course are:\n",
        "\n",
        "Genesis: It is a collection of few words across multiple languages.\n",
        "\n",
        "Brown: It is the first electronic corpus of one million English words.\n",
        "\n",
        "Other Corpus in nltk\n",
        "\n",
        "Gutenberg : Collections from Project Gutenberg\n",
        "Inaugural : Collection of U.S Presidents inaugural speeches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WF8h_HGGfo53"
      },
      "source": [
        "from nltk.book import *\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbqqKdiefo2a",
        "outputId": "533acf42-4697-42df-ab7a-f69fe7b60f28"
      },
      "source": [
        "text1.findall(\"<tri.*r>\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "triangular; triangular; triangular; triangular\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFvrGs4vooQR"
      },
      "source": [
        "from nltk.corpus import genesis"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmjqYz_EooM-",
        "outputId": "31123dd6-32a5-4750-a77f-c85208350e2f"
      },
      "source": [
        "genesis.fileids()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['english-kjv.txt',\n",
              " 'english-web.txt',\n",
              " 'finnish.txt',\n",
              " 'french.txt',\n",
              " 'german.txt',\n",
              " 'lolcat.txt',\n",
              " 'portuguese.txt',\n",
              " 'swedish.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXRAPOHTooKS",
        "outputId": "575e657c-bb76-4b51-ae45-cc0e0eb66218"
      },
      "source": [
        "for fileid in genesis.fileids():\n",
        "...    n_chars = len(genesis.raw(fileid))\n",
        "...    n_words = len(genesis.words(fileid))\n",
        "...    n_sents = len(genesis.sents(fileid))\n",
        "...    print(int(n_chars/n_words), int(n_words/n_sents), fileid)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 30 english-kjv.txt\n",
            "4 19 english-web.txt\n",
            "5 15 finnish.txt\n",
            "4 23 french.txt\n",
            "4 23 german.txt\n",
            "4 20 lolcat.txt\n",
            "4 27 portuguese.txt\n",
            "4 30 swedish.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7bq2BvyooHZ",
        "outputId": "b92b956a-e535-42f1-8418-5f626273a55e"
      },
      "source": [
        "from nltk.corpus import PlaintextCorpusReader\n",
        "corpus_root = '/usr/share/dict'\n",
        "wordlists = PlaintextCorpusReader(corpus_root, '.*')\n",
        "wordlists.fileids()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zJQJERCooEP"
      },
      "source": [
        ""
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRNh2VteopQM"
      },
      "source": [
        "##cnf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8DqhuHb2fZnh",
        "outputId": "883de86e-1ac3-4305-8e2f-f954772a2b02"
      },
      "source": [
        "items = ['apple', 'apple', 'kiwi', 'cabbage', 'cabbage', 'potato']\n",
        "nltk.FreqDist(items)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({'apple': 2, 'cabbage': 2, 'kiwi': 1, 'potato': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsJ-JS1Wfjde"
      },
      "source": [
        "c_items = [('F','apple'), ('F','apple'), ('F','kiwi'), ('V','cabbage'), ('V','cabbage'), ('V','potato') ]"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dcsw8uqef6UL",
        "outputId": "6e10d087-c949-486f-ea1b-2201cd84f60c"
      },
      "source": [
        "cfd = nltk.ConditionalFreqDist(c_items)\n",
        "cfd.conditions()\n",
        "#['V', 'F']\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['F', 'V']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VuykZuW7gC0B",
        "outputId": "60b2b240-0f66-4632-d8f4-e7b6cc2ddbc6"
      },
      "source": [
        "cfd['V']\n",
        "#FreqDist({'cabbage': 2, 'potato': 1})\n",
        "#cfd['F']\n",
        "#FreqDist({'apple': 2, 'kiwi': 1})"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({'cabbage': 2, 'potato': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJC5LE0UheNy"
      },
      "source": [
        "from nltk.corpus import reuters"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mkiHOUt_v2B8",
        "outputId": "11c301e9-dd9a-4a97-ab84-c1a659051e0f"
      },
      "source": [
        "nltk.download('reuters')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RBZejmI7gHPO",
        "outputId": "20465723-2ff0-4cdf-b119-7a5664f0677c"
      },
      "source": [
        "cfd = nltk.ConditionalFreqDist([ (genre, word) for genre in reuters.categories() for word in reuters.words(categories=genre) ])\n",
        "\n",
        "cfd.conditions()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mreuters\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('reuters')\n  \u001b[0m\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/lib/nltk_data'\n**********************************************************************\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-26224905376e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConditionalFreqDist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgenre\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mgenre\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreuters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreuters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenre\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcfd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconditions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mreuters\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('reuters')\n  \u001b[0m\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1H6n-WRgUsD"
      },
      "source": [
        "cfd.tabulate(conditions=['government', 'humor', 'reviews'], samples=['leadership', 'worship', 'hardship'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfoL7_0sgfHX"
      },
      "source": [
        "cfd.tabulate(conditions=['government', 'humor', 'reviews'], samples=['leadership', 'worship', 'hardship'], cumulative = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swCHYLUOgiQ_"
      },
      "source": [
        "news_fd = cfd['zinc']\n",
        "news_fd.most_common(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEcgRNT-gpLw"
      },
      "source": [
        "news_fd['zinc']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10ufts5wgrZe"
      },
      "source": [
        "#The text corpus names contain two files male.txt and female.txt.\n",
        "from nltk.corpus import names\n",
        "nt = [(fid.split('.')[0], name[-1])    for fid in names.fileids()  for name in names.words(fid) ]\n",
        "cfd2 = nltk.ConditionalFreqDist(nt)\n",
        "cfd2['female'] > cfd2['male']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPz_SLuwg7Rg"
      },
      "source": [
        "cfd2.tabulate(samples=['a', 'e'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbN4Envpg-j5"
      },
      "source": [
        "items = [\"zinc\"]\n",
        "nltk.FreqDist(items)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibpthTCGjNMv"
      },
      "source": [
        "##raw text processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-P8I1LPiQgi"
      },
      "source": [
        "from urllib import request\n",
        "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
        "content1 = request.urlopen(url).read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whdG5lIsjVtw"
      },
      "source": [
        "from urllib import request\n",
        "\n",
        "url = \"http://www.bbc.com/news/health-42802191\"\n",
        "html_content = request.urlopen(url).read()\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "soup = BeautifulSoup(html_content, 'html.parser')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJm6JIh0jcya"
      },
      "source": [
        "inner_body = soup.find_all('div', attrs={'class':'story-body__inner'})\n",
        "\n",
        "inner_text = [elm.text for elm in inner_body[0].find_all(['h1', 'h2', 'p', 'li']) ]\n",
        " \n",
        "text_content2 = '\\n'.join(inner_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HlqRtL4jhRo"
      },
      "source": [
        "#Third party libraries such as pywin32, pypdf are required for accessing Microsoft Word or PDF documents.\n",
        "text_content1 = content1.decode('unicode_escape')  # Converts bytes to unicode\n",
        "tokens1 = nltk.word_tokenize(text_content1)\n",
        "tokens1[3:8]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyGTB2xBj1eh"
      },
      "source": [
        "tokens2 = nltk.word_tokenize(text_content2)\n",
        "tokens2[:5]\n",
        "#['Smokers', 'need', 'to', 'quit', 'cigarettes']\n",
        "#len(tokens2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLUm5EW5kFNP"
      },
      "source": [
        "tokens2_2 = re.findall(r'\\w+', text_content2)\n",
        "len(tokens2_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_rB8gpOkKs4"
      },
      "source": [
        "pattern = r'\\w+'\n",
        "tokens2_3 = nltk.regexp_tokenize(text_content2, pattern)\n",
        "len(tokens2_3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zkjix4a1kSY5"
      },
      "source": [
        "input_text2 = nltk.Text(tokens2)\n",
        "type(input_text2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5_b_FbYhpDQ"
      },
      "source": [
        "##bigrams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaYlKnfEkWCy"
      },
      "source": [
        "#n-grams\n",
        "\n",
        "import nltk\n",
        "s = 'Python is an awesome language.'\n",
        "tokens = nltk.word_tokenize(s)\n",
        "list(nltk.bigrams(tokens))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svtwP7Pfk9VR"
      },
      "source": [
        "from nltk.corpus import genesis\n",
        "eng_tokens = genesis.words('english-kjv.txt')\n",
        "eng_bigrams = nltk.bigrams(eng_tokens)\n",
        "filtered_bigrams = [ (w1, w2) for w1, w2 in eng_bigrams if len(w1) >=5 and len(w2) >= 5 ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ytaOR9rlC7Q"
      },
      "source": [
        "eng_bifreq = nltk.FreqDist(filtered_bigrams)\n",
        "eng_bifreq.most_common(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhpJb7zplXyx"
      },
      "source": [
        "from nltk.corpus import genesis\n",
        "eng_tokens = genesis.words('english-kjv.txt')\n",
        "eng_bigrams = nltk.bigrams(eng_tokens)\n",
        "eng_cfd = nltk.ConditionalFreqDist(eng_bigrams)\n",
        "eng_cfd['living'].most_common(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUGdfNI_leWl"
      },
      "source": [
        "def generate(cfd, word, n=5):\n",
        "   n_words = []\n",
        "   for i in range(n):\n",
        "    n_words.append(word)\n",
        "    word = cfd[word].max()\n",
        "    return n_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZpWyI95lnqA"
      },
      "source": [
        "generate(eng_cfd, 'living')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LB-O1ojLlvSX"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0bxvz_Bcf9_"
      },
      "source": [
        "## nGRAMS AND cOLLOCATIONS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvViaMNslq9I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a70b7b6d-b1a2-4ce6-f147-2cc77689a1ec"
      },
      "source": [
        "s = 'Python is an awesome language.'\n",
        "d=\"Python is cool!!!\"\n",
        "tokens = nltk.word_tokenize(d)\n",
        "list(nltk.trigrams(tokens))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Python', 'is', 'cool'),\n",
              " ('is', 'cool', '!'),\n",
              " ('cool', '!', '!'),\n",
              " ('!', '!', '!')]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7sarzN4gRC-"
      },
      "source": [
        "list(nltk.bigrams(tokens))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OWMqPOGl0tw"
      },
      "source": [
        "list(nltk.ngrams(tokens, 4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NwanqJp0clXk"
      },
      "source": [
        "eng_tokens = genesis.words(\"english-kjv.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKJ6IYP5d6bh"
      },
      "source": [
        "eng_bigrams = nltk.bigrams(eng_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M14DHe9jeClH"
      },
      "source": [
        "filtered_bigrams= [ (w1, w2) for w1, w2 in eng_bigrams if len(w1)>=5 and len(w2)>=5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJNOmiNmeajD"
      },
      "source": [
        "eng_bifreq = nltk.FreqDist(filtered_bigrams)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlJ4Suz6elQX"
      },
      "source": [
        "eng_bifreq.most_common(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxu7mmS5eqnL"
      },
      "source": [
        "from nltk.corpus import genesis\n",
        "eng_tokens = genesis.words(\"english-kjv.txt\")\n",
        "eng_bigrams = nltk.bigrams(eng_tokens)\n",
        "eng_cfd = nltk.ConditionalFreqDist(eng_bigrams)\n",
        "eng_cfd[\"living\"].most_common(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cmv4aFyfXA4"
      },
      "source": [
        "def generate(cfd, word, n=5):\n",
        "  n_words = []\n",
        "  for i in range(n):\n",
        "    n_words.append(word)\n",
        "    word = cfd[word].max()\n",
        "  return n_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XQ0F_enf7WB"
      },
      "source": [
        "generate(eng_cfd, \"living\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46K4jjFRgAbc"
      },
      "source": [
        "from nltk.corpus import genesis\n",
        "tokens = genesis.words(\"english-kjv.txt\")\n",
        "gen_text = nltk.Text(tokens)\n",
        "gen_text.collocations()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfWzP9prgrbl"
      },
      "source": [
        "from nltk.book import *\n",
        "text6[:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ny-DJcOhibV7"
      },
      "source": [
        "#tokens = genesis.words(\"text6\")\n",
        "gen_text = nltk.Text(text6)\n",
        "gen_text.collocations()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKggdwPein4A"
      },
      "source": [
        "eng_bigrams = nltk.bigrams(text6)\n",
        "eng_cfd = nltk.ConditionalFreqDist(eng_bigrams)\n",
        "eng_cfd[\"FRENCH\"].most_common()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEoiorXwkR1L"
      },
      "source": [
        "##Stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RVV6EaNju7q"
      },
      "source": [
        "from nltk import PorterStemmer\n",
        "\n",
        "porter = nltk.PorterStemmer()\n",
        "porter.stem(\"ceremony\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9D0RDTVk6p6"
      },
      "source": [
        "#normalizing with stemming\n",
        "\n",
        "from nltk.book import *\n",
        "len(set(text1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PjVF1DKlKxo"
      },
      "source": [
        "lc_words = [ word.lower() for word in text6]\n",
        "len(set(lc_words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhQnzpOJlj_G"
      },
      "source": [
        "p_stem_words = [ porter.stem(word) for word in set(lc_words)]\n",
        "len(set(p_stem_words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yoE21Yckqyq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "66adc8ea-248c-4125-dd89-f1f2e439045f"
      },
      "source": [
        "from nltk import LancasterStemmer\n",
        "\n",
        "lanc = nltk.LancasterStemmer()\n",
        "lanc.stem(\"builders\")"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'build'"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XowExP2vm87M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a2e98fd7-11c1-43a2-ada7-c2bf34280c04"
      },
      "source": [
        "lanc.stem(\"lying\")"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'lying'"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07OAXwnIlXGK"
      },
      "source": [
        "l_stem_words = [ lanc.stem(word) for word in set(lc_words)]\n",
        "len(set(l_stem_words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FsbB4dfmNw6"
      },
      "source": [
        "wnl = nltk.WordNetLemmatizer()\n",
        "wnl_stem_words = [wnl.lemmatize(word) for word in set(lc_words)]\n",
        "len(set(wnl_stem_words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8M3j5Trm0U4"
      },
      "source": [
        "#find words ending with \"ly\" and \"ing\"\n",
        "import re\n",
        "word = re.findall(r'\\b\\w+(ing\\b)', text6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Df7bMXHWo7CM"
      },
      "source": [
        "##Pos Tagger"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzY1sL28n-5U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "411893d3-ea7e-4b4f-bdc5-75f253db9831"
      },
      "source": [
        "s = 'Python is an awesome language.'\n",
        "d=\"Python is cool!!!\"\n",
        "tokens = nltk.word_tokenize(d)\n",
        "list(nltk.trigrams(tokens))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Python', 'is', 'cool'),\n",
              " ('is', 'cool', '!'),\n",
              " ('cool', '!', '!'),\n",
              " ('!', '!', '!')]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fr45yi4-wey3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b2c8aae-abbc-40b6-8178-7bc73dd11ecd"
      },
      "source": [
        "import re\n",
        "print(re.findall(r'\\w+', d))\n",
        "#print(re.findall(r'\\s\\w+\\b', d))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Python', 'is', 'cool']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blBirumjpGXv"
      },
      "source": [
        "nltk.pos_tag(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XDTvUHLqUoh"
      },
      "source": [
        "default_tagger = nltk.DefaultTagger(\"NN\")\n",
        "default_tagger.tag(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkGhp1ocqoJD"
      },
      "source": [
        "#defined tags\n",
        "defined_tags = {\"is\": \"BEZ\", \"over\": \"IN\", \"who\":\"WPS\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiPwSl2Lq7ic"
      },
      "source": [
        "#BASELINE TAGGER\n",
        "base =nltk.UnigramTagger(model=defined_tags)\n",
        "base.tag(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AO6tfl8KpNzS"
      },
      "source": [
        "#nltk.help.upenn_tagset()\n",
        "nltk.help.upenn_tagset(\"JJ\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOmGjtdjpXqE"
      },
      "source": [
        "text = \"Python/NN is/VB\"\n",
        "[nltk.tag.str2tuple(word) for word in text.split() ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2XYriI9p-F7"
      },
      "source": [
        "from nltk.corpus import brown\n",
        "brown_tagged = brown.tagged_words()\n",
        "brown_tagged[:3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4DxCrh-qPgQ"
      },
      "source": [
        "brown_tagged_sents = brown.tagged_sents(categories=\"government\")\n",
        "brown_sents = brown.sents(categories=\"government\")\n",
        "len(brown_sents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKFdJPLgrqK5"
      },
      "source": [
        "train_size = int(len(brown_sents)*0.8)\n",
        "train_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suLaQpqcsFf2"
      },
      "source": [
        "train_sents = brown_tagged_sents[:train_size]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdgfXcW0sQRz"
      },
      "source": [
        "test_sents = brown_tagged_sents[train_size:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eueozx1IsagS"
      },
      "source": [
        "unigram_tagger = nltk.UnigramTagger(train_sents)\n",
        "unigram_tagger.evaluate(test_sents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pyt8kvJ3svla"
      },
      "source": [
        "unigram_tagger.tag(brown_sents[3000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## eXTRA"
      ],
      "metadata": {
        "id": "ceTo6TDYLzfD"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VU_UHJbus4_P"
      },
      "source": [
        "# processes file.txt and prints all the words ending in ing\n",
        "for line in open(\"file.txt\"):\n",
        "     for word in line.split():\n",
        "         if word.endswith('ing'):\n",
        "             print word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section-2"
      ],
      "metadata": {
        "id": "Eq-8fJ9DilN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download()"
      ],
      "metadata": {
        "id": "JAmBrTwpiomQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.book import *"
      ],
      "metadata": {
        "id": "efrhiwR2iuNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text1"
      ],
      "metadata": {
        "id": "wDLoqW1Ji7-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#A concordance view shows us every occurrence of a given word, together with some context\n",
        "text1.concordance(\"monstrous\")"
      ],
      "metadata": {
        "id": "JgBF_msfyvnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text1.similar(\"monstrous\")"
      ],
      "metadata": {
        "id": "Xh2CuVcpy8E5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The term common_contexts allows us to examine just the contexts that are shared by two or more words, such as monstrous and very\n",
        "text2.common_contexts([\"monstrous\", \"very\"])"
      ],
      "metadata": {
        "id": "g97-DR_BzFvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#positional information\n",
        "#location of a word\n",
        "text4.dispersion_plot([\"citizens\", \"democracy\", \"freedom\", \"duties\", \"America\"])"
      ],
      "metadata": {
        "id": "aicp-OKwzXkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text3.generate()"
      ],
      "metadata": {
        "id": "xdl0rd7szfbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## counting vocabs"
      ],
      "metadata": {
        "id": "3vSdjtlv0WSV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(text3)"
      ],
      "metadata": {
        "id": "xlw-zkKfzz9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted(set(text3))"
      ],
      "metadata": {
        "id": "7z8y6sRO0K9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(set(text3))"
      ],
      "metadata": {
        "id": "xFtp-JGq0OzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate a measure of the lexical richness of the text\n",
        "from __future__ import division\n",
        "len(text3) / len(set(text3))"
      ],
      "metadata": {
        "id": "7gV-99eR0Scr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#particular words\n",
        "text3.count(\"smote\")"
      ],
      "metadata": {
        "id": "VHut_N4c1-PN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "100 * text4.count('a') / len(text4)"
      ],
      "metadata": {
        "id": "HCUOks5w2G0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text5.count(\"lol\")"
      ],
      "metadata": {
        "id": "mBWy8le725Vx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "100 * text4.count('a') / len(text4)"
      ],
      "metadata": {
        "id": "g8mlleSc3B3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#How many times does the word lol appear in text5?\n",
        "# How much is this as a percentage of the total number of words in this text?\n",
        "def lexical_diversity(text): \n",
        "    return len(text) / len(set(text))\n",
        "\n",
        "def percentage(count, total):\n",
        "    return 100 * count / total"
      ],
      "metadata": {
        "id": "sjpRc49R2JUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lexical_diversity(text3)"
      ],
      "metadata": {
        "id": "H41gIIEP2mqs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lexical_diversity(text5)"
      ],
      "metadata": {
        "id": "d5CtWZIQ2uH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "percentage(4, 5)"
      ],
      "metadata": {
        "id": "7RB9op_G2xFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "percentage(text4.count('a'), len(text4))"
      ],
      "metadata": {
        "id": "Jgl2ZmDw2zl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lexical Diversity of Various Genres in the Brown Corpus\n",
        "\n",
        "Genre\tTokens\tTypes\tLexical diversity\n",
        "\n",
        "skill and hobbies\t82345\t11935\t6.9\n",
        "\n",
        "humor\t21695\t5017\t4.3\n",
        "\n",
        "fiction: science\t14470\t3233\t4.5\n",
        "\n",
        "press: reportage\t100554\t14394\t7.0\n",
        "\n",
        "fiction: romance\t70022\t8452\t8.3\n",
        "\n",
        "religion\t39399\t6373\t6.2"
      ],
      "metadata": {
        "id": "rsqXnCXn3M6R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Texts as Lists of Words"
      ],
      "metadata": {
        "id": "5sc--cH03cxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sent1 = ['Call', 'me', 'Ishmael', '.']\n",
        "sent1"
      ],
      "metadata": {
        "id": "EI2R7hAC22Ev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(sent1)"
      ],
      "metadata": {
        "id": "nkuajo3S3jCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lexical_diversity(sent1)"
      ],
      "metadata": {
        "id": "dR5sdLkM3k5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent2"
      ],
      "metadata": {
        "id": "FcyyegHr3nX0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent3"
      ],
      "metadata": {
        "id": "0yG6nRdj3sMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make up a few sentences of your own, by typing a name, equals sign, and a list of words, like this: ex1 = ['Monty', 'Python', 'and', 'the', 'Holy', 'Grail']. Repeat some of the other Python operations we saw earlier in 1.1, e.g., sorted(ex1), len(set(ex1)), ex1.count('the')"
      ],
      "metadata": {
        "id": "-vFK_hzQ32ze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sent4 + sent1"
      ],
      "metadata": {
        "id": "s6sq-2FZ3vDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent1.append(\"Some\")\n",
        "sent1"
      ],
      "metadata": {
        "id": "ccn6YE4Z4AnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Indexing"
      ],
      "metadata": {
        "id": "5y5xICBT4GX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text4[173]"
      ],
      "metadata": {
        "id": "P9V5fIY34SQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text4.index('awaken')"
      ],
      "metadata": {
        "id": "XMB5I7g-4S3G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#slicing\n",
        "text6[1600:1625]"
      ],
      "metadata": {
        "id": "uTOuvUn_4Vk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent = ['word1', 'word2', 'word3', 'word4', 'word5',\n",
        "        'word6', 'word7', 'word8', 'word9', 'word10']"
      ],
      "metadata": {
        "id": "L52sc-DY4ake"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent[9]"
      ],
      "metadata": {
        "id": "A3hfhcBA4g1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent[5:8]"
      ],
      "metadata": {
        "id": "Z9t12ap44ipu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent[:3]"
      ],
      "metadata": {
        "id": "9hAGU3U34uGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text2[141565:]"
      ],
      "metadata": {
        "id": "RqUgSWrp41-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##variables"
      ],
      "metadata": {
        "id": "Xgad5C7j45YX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent1 = ['Call', 'me', 'Ishmael', '.']"
      ],
      "metadata": {
        "id": "aFQHvzwv5KXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_sent = ['Bravely', 'bold', 'Sir', 'Robin', ',', 'rode',\n",
        " 'forth', 'from', 'Camelot', '.']\n",
        "noun_phrase = my_sent[1:4]\n",
        "noun_phrase"
      ],
      "metadata": {
        "id": "e0GQM2jm5MvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wOrDs = sorted(noun_phrase)\n",
        "wOrDs"
      ],
      "metadata": {
        "id": "LlGUP5tX5WxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = set(text1)\n",
        "vocab_size = len(vocab)\n",
        "vocab_size"
      ],
      "metadata": {
        "id": "VMD7O66A5gVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "name = 'Monty'\n",
        "' '.join(['Monty', 'Python'])"
      ],
      "metadata": {
        "id": "jG4C0Gqm5y1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'Monty Python'.split()"
      ],
      "metadata": {
        "id": "RNpDil4p6Coi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple Statistics with text"
      ],
      "metadata": {
        "id": "3ev7kFBj6L-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "saying = ['After', 'all', 'is', 'said', 'and', 'done',\n",
        "           'more', 'is', 'said', 'than', 'done']"
      ],
      "metadata": {
        "id": "vk1Gz-vf6F4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = set(saying)\n",
        "tokens = sorted(tokens)\n",
        "tokens[-2:]"
      ],
      "metadata": {
        "id": "1qP6mFBc8Yxe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Frequency Distributions\n",
        "fdist1 = FreqDist(text1)   #inspect the total number of words (\"outcomes\") that have been counted up\n",
        "fdist1"
      ],
      "metadata": {
        "id": "_yC0k4zN8f4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fdist1.hapaxes()   #the words that occur once only, the so-called hapaxes"
      ],
      "metadata": {
        "id": "S-D_x0FW-F23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary1 = fdist1.keys() #expression keys() gives us a list of all the distinct types in the text\n",
        "vocabulary1"
      ],
      "metadata": {
        "id": "gzV8sTi387XD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fdist1['whale']"
      ],
      "metadata": {
        "id": "GX74N7Ga9DzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fine-grained Selection of Words\n",
        "\n"
      ],
      "metadata": {
        "id": "ud5srSXC9MXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "V = set(text1)\n",
        "long_words = [w for w in V if len(w) > 15]\n",
        "sorted(long_words)"
      ],
      "metadata": {
        "id": "_9XPoWDE-T8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fdist5 = FreqDist(text5)\n",
        "sorted([w for w in set(text5) if len(w) > 7 and fdist5[w] > 7])"
      ],
      "metadata": {
        "id": "_hSsuxUz-apb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Collocations and Bigrams"
      ],
      "metadata": {
        "id": "Gc_u8Oit-pR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#A collocation is a sequence of words that occur together unusually often\n",
        "x = bigrams(['more', 'is', 'said', 'than', 'done'])\n",
        "x"
      ],
      "metadata": {
        "id": "Csn60mwN-ixb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text4.collocations()"
      ],
      "metadata": {
        "id": "lTp_THIV-3pJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#deriving a list of the lengths of words in text1\n",
        "[len(w) for w in text1]"
      ],
      "metadata": {
        "id": "LyMb2brg-51r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the FreqDist then counts the number of times each of these occurs\n",
        "fdist = FreqDist([len(w) for w in text1])\n",
        "fdist"
      ],
      "metadata": {
        "id": "AaJRZD-z_NYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fdist.keys()"
      ],
      "metadata": {
        "id": "4RAplrLH_Y07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fdist.items()"
      ],
      "metadata": {
        "id": "y3U1yZEI_cQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fdist.max()"
      ],
      "metadata": {
        "id": "qL8Pd5_q_fTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fdist[3]"
      ],
      "metadata": {
        "id": "LBQv7Dy4_iW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fdist.freq(3)"
      ],
      "metadata": {
        "id": "RyksH4u7_kRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some Word Comparison Operators\n",
        "\n",
        "Function\tMeaning\n",
        "\n",
        "s.startswith(t)\ttest if s starts with t\n",
        "\n",
        "s.endswith(t)\ttest if s ends with t\n",
        "\n",
        "t in s\ttest if t is contained inside s\n",
        "\n",
        "s.islower()\ttest if all cased characters in s are lowercase\n",
        "\n",
        "s.isupper()\ttest if all cased characters in s are uppercase\n",
        "\n",
        "s.isalpha()\ttest if all characters in s are alphabetic\n",
        "\n",
        "s.isalnum()\ttest if all characters in s are alphanumeric\n",
        "\n",
        "s.isdigit()\ttest if all characters in s are digits\n",
        "\n",
        "s.istitle()\ttest if s is titlecased (all words in s have have initial capitals)"
      ],
      "metadata": {
        "id": "xtQ8PEEf_wTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#[w for w in text if condition ]\n",
        "\n",
        "sorted([w for w in set(text1) if w.endswith('ableness')])"
      ],
      "metadata": {
        "id": "D7bveN6U_mwE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted([term for term in set(text4) if 'gnt' in term])"
      ],
      "metadata": {
        "id": "-QORB7hDAAo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted([item for item in set(text6) if item.istitle()])"
      ],
      "metadata": {
        "id": "9CLqpwqsAD4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted([item for item in set(sent7) if item.isdigit()])"
      ],
      "metadata": {
        "id": "Q4qpEmbDAGru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sorted([w for w in set(text7) if '-' in w and 'index' in w])\n",
        "sorted([wd for wd in set(text3) if wd.istitle() and len(wd) > 10])\n",
        "sorted([w for w in set(sent7) if not w.islower()])\n",
        "sorted([t for t in set(text2) if 'cie' in t or 'cei' in t])"
      ],
      "metadata": {
        "id": "wSpF9F9DAKRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#some examples of counting items other than words\n",
        "[len(w) for w in text1]"
      ],
      "metadata": {
        "id": "Jhe0X01uASG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[w.upper() for w in text1]"
      ],
      "metadata": {
        "id": "aaVETjJ_Ab31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#vocabulary size\n",
        "len(text1)\n",
        "len(set(text1))\n",
        "len(set([word.lower() for word in text1]))"
      ],
      "metadata": {
        "id": "zN1Zq32mAeCU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#vocabulary count by filtering out any non-alphabetic items\n",
        "len(set([word.lower() for word in text1 if word.isalpha()]))"
      ],
      "metadata": {
        "id": "9t5M3-t6AtAl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### control structures"
      ],
      "metadata": {
        "id": "OZyD0r6TBMQ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ">>> word = 'cat'\n",
        ">>> if len(word) < 5:\n",
        "...     print('word length is less than 5')"
      ],
      "metadata": {
        "id": "fcjSIlrOA5hW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ">>> if len(word) >= 5:\n",
        "...   print('word length is greater than or equal to 5')"
      ],
      "metadata": {
        "id": "G4W2GgGdA_92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ">>> for word in ['Call', 'me', 'Ishmael', '.']:\n",
        "...     print(word)"
      ],
      "metadata": {
        "id": "l7AsoG0vBLEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Looping with Conditions\n",
        "\n",
        "sent1 = ['Call', 'me', 'Ishmael', '.']\n",
        "for xyzzy in sent1:\n",
        "     if xyzzy.endswith('l'):\n",
        "         print(xyzzy)"
      ],
      "metadata": {
        "id": "ErW1bi-ZBTs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token in sent1:\n",
        "    if token.islower():\n",
        "        print(token, 'is a lowercase word')\n",
        "    elif token.istitle():\n",
        "        print(token, 'is a titlecase word')\n",
        "    else:\n",
        "        print(token, 'is punctuation')"
      ],
      "metadata": {
        "id": "iE9FDFoiBbme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tricky = sorted([w for w in set(text2) if 'cie' in w or 'cei' in w])\n",
        "for word in tricky:\n",
        "    print(word,)"
      ],
      "metadata": {
        "id": "YkyJ40NmBy89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Accessing Text Corpora and Lexical Resources"
      ],
      "metadata": {
        "id": "fJGfio2pCWZ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.corpus.gutenberg.fileids()"
      ],
      "metadata": {
        "id": "Iq32OoSWCI4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emma = nltk.corpus.gutenberg.words('austen-emma.txt')\n",
        "len(emma)"
      ],
      "metadata": {
        "id": "_J6NsxPKDcAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emma = nltk.Text(nltk.corpus.gutenberg.words('austen-emma.txt'))\n",
        "emma.concordance(\"surprize\")"
      ],
      "metadata": {
        "id": "6CaQpvjFDhTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import gutenberg\n",
        "gutenberg.fileids()"
      ],
      "metadata": {
        "id": "3E1NPTnUDopM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emma = gutenberg.words('austen-emma.txt')"
      ],
      "metadata": {
        "id": "GWp0Gaz5DymR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for fileid in gutenberg.fileids():\n",
        "     num_chars = len(gutenberg.raw(fileid))\n",
        "     num_words = len(gutenberg.words(fileid))\n",
        "     num_sents = len(gutenberg.sents(fileid))\n",
        "     num_vocab = len(set([w.lower() for w in gutenberg.words(fileid)]))\n",
        "     print(int(num_chars/num_words), int(num_words/num_sents), int(num_words/num_vocab), fileid)"
      ],
      "metadata": {
        "id": "n_sp3oHCD2FP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "macbeth_sentences = gutenberg.sents('shakespeare-macbeth.txt')\n",
        "macbeth_sentences"
      ],
      "metadata": {
        "id": "4Vrih1veECTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "macbeth_sentences[1037]"
      ],
      "metadata": {
        "id": "Wr6p4An6EIpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "longest_len = max([len(s) for s in macbeth_sentences])\n",
        "[s for s in macbeth_sentences if len(s) == longest_len]"
      ],
      "metadata": {
        "id": "qYCamvWnELXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Most NLTK corpus readers include a variety of access methods apart from words(), raw(), and sents()"
      ],
      "metadata": {
        "id": "VV7FRuS7EPbZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import webtext\n",
        "for fileid in webtext.fileids():\n",
        "     print(fileid, webtext.raw(fileid)[:65], '...')"
      ],
      "metadata": {
        "id": "lBHOQy53EXYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import nps_chat\n",
        "chatroom = nps_chat.posts('10-19-20s_706posts.xml')\n",
        "chatroom[123]"
      ],
      "metadata": {
        "id": "4D4AK-OMEeJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Brown Corpus\n",
        "from nltk.corpus import brown\n",
        "brown.categories()"
      ],
      "metadata": {
        "id": "TiAlVhdGEl3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "brown.words(categories='news')"
      ],
      "metadata": {
        "id": "uzjg3CxcEv0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "brown.words(fileids=['cg22'])"
      ],
      "metadata": {
        "id": "ah_fCvs9Ey89"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "brown.sents(categories=['news', 'editorial', 'reviews'])"
      ],
      "metadata": {
        "id": "tzsNXdPXE1Og"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import brown   #differences between genres, a kind of linguistic inquiry known as stylistics\n",
        "news_text = brown.words(categories='news')\n",
        "fdist = nltk.FreqDist([w.lower() for w in news_text])\n",
        "modals = ['can', 'could', 'may', 'might', 'must', 'will']\n",
        "for m in modals:\n",
        "    print(m + ':', fdist[m],)"
      ],
      "metadata": {
        "id": "Ebr3I-8WE4GS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Choose a different section of the Brown Corpus, and adapt the previous example to count\n",
        "# a selection of wh words, such as what, when, where, who, and why."
      ],
      "metadata": {
        "id": "yi7neiOlFBva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfd = nltk.ConditionalFreqDist(\n",
        "          (genre, word)\n",
        "          for genre in brown.categories()\n",
        "          for word in brown.words(categories=genre))\n",
        "genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor']\n",
        "modals = ['can', 'could', 'may', 'might', 'must', 'will']\n",
        "cfd.tabulate(conditions=genres, samples=modals)"
      ],
      "metadata": {
        "id": "cqCEZ2OEFT2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Reuters Corpus\n",
        "#The Reuters Corpus contains 10,788 news documents totaling 1.3 million words.\n",
        "# The documents have been classified into 90 topics, and grouped into two sets,\n",
        "# called \"training\" and \"test\"; thus, the text with fileid 'test/14826'\n",
        "\n"
      ],
      "metadata": {
        "id": "gvw_RvFIFbnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('reuters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6D-4mQsqtBTW",
        "outputId": "ef0aa00e-d624-4a23-ff90-96905da7a9f4"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import reuters\n",
        "reuters.fileids()\n",
        "reuters.categories()\n",
        "reuters.categories('training/9865')\n",
        "reuters.categories(['training/9865', 'training/9880'])\n",
        "reuters.fileids('barley')\n",
        "reuters.fileids(['barley', 'corn'])\n",
        "\n",
        "reuters.words('training/9865')[:14]\n",
        "reuters.words(['training/9865', 'training/9880'])\n",
        "reuters.words(categories='barley')\n",
        "reuters.words(categories=['barley', 'corn'])"
      ],
      "metadata": {
        "id": "wsHQaZqDFtkU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "28c81032-c4a2-4dce-d7f1-7ef305d3f900"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mreuters\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('reuters')\n  \u001b[0m\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/lib/nltk_data'\n**********************************************************************\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-ffab1c159fc6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreuters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mreuters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mreuters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mreuters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'training/9865'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mreuters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'training/9865'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'training/9880'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mreuters\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('reuters')\n  \u001b[0m\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import inaugural\n",
        "inaugural.fileids()"
      ],
      "metadata": {
        "id": "zrYJGiDrGOPj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "299bb451-45ef-4927-977b-b656e3c00c3f"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1789-Washington.txt',\n",
              " '1793-Washington.txt',\n",
              " '1797-Adams.txt',\n",
              " '1801-Jefferson.txt',\n",
              " '1805-Jefferson.txt',\n",
              " '1809-Madison.txt',\n",
              " '1813-Madison.txt',\n",
              " '1817-Monroe.txt',\n",
              " '1821-Monroe.txt',\n",
              " '1825-Adams.txt',\n",
              " '1829-Jackson.txt',\n",
              " '1833-Jackson.txt',\n",
              " '1837-VanBuren.txt',\n",
              " '1841-Harrison.txt',\n",
              " '1845-Polk.txt',\n",
              " '1849-Taylor.txt',\n",
              " '1853-Pierce.txt',\n",
              " '1857-Buchanan.txt',\n",
              " '1861-Lincoln.txt',\n",
              " '1865-Lincoln.txt',\n",
              " '1869-Grant.txt',\n",
              " '1873-Grant.txt',\n",
              " '1877-Hayes.txt',\n",
              " '1881-Garfield.txt',\n",
              " '1885-Cleveland.txt',\n",
              " '1889-Harrison.txt',\n",
              " '1893-Cleveland.txt',\n",
              " '1897-McKinley.txt',\n",
              " '1901-McKinley.txt',\n",
              " '1905-Roosevelt.txt',\n",
              " '1909-Taft.txt',\n",
              " '1913-Wilson.txt',\n",
              " '1917-Wilson.txt',\n",
              " '1921-Harding.txt',\n",
              " '1925-Coolidge.txt',\n",
              " '1929-Hoover.txt',\n",
              " '1933-Roosevelt.txt',\n",
              " '1937-Roosevelt.txt',\n",
              " '1941-Roosevelt.txt',\n",
              " '1945-Roosevelt.txt',\n",
              " '1949-Truman.txt',\n",
              " '1953-Eisenhower.txt',\n",
              " '1957-Eisenhower.txt',\n",
              " '1961-Kennedy.txt',\n",
              " '1965-Johnson.txt',\n",
              " '1969-Nixon.txt',\n",
              " '1973-Nixon.txt',\n",
              " '1977-Carter.txt',\n",
              " '1981-Reagan.txt',\n",
              " '1985-Reagan.txt',\n",
              " '1989-Bush.txt',\n",
              " '1993-Clinton.txt',\n",
              " '1997-Clinton.txt',\n",
              " '2001-Bush.txt',\n",
              " '2005-Bush.txt',\n",
              " '2009-Obama.txt',\n",
              " '2013-Obama.txt',\n",
              " '2017-Trump.txt',\n",
              " '2021-Biden.txt']"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[fileid[:4] for fileid in inaugural.fileids()]"
      ],
      "metadata": {
        "id": "Y-v-2No0GYvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfd = nltk.ConditionalFreqDist(\n",
        "          (target, fileid[:4])\n",
        "          for fileid in inaugural.fileids()\n",
        "          for w in inaugural.words(fileid)\n",
        "          for target in ['america', 'citizen']\n",
        "          if w.lower().startswith(target))\n",
        "cfd.plot()"
      ],
      "metadata": {
        "id": "6h6hpy62GbhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ">>> from nltk.corpus import udhr\n",
        ">>> languages = ['Chickasaw', 'English', 'German_Deutsch',\n",
        "...     'Greenlandic_Inuktikut', 'Hungarian_Magyar', 'Ibibio_Efik']\n",
        ">>> cfd = nltk.ConditionalFreqDist(\n",
        "...           (lang, len(word))\n",
        "...           for lang in languages\n",
        "...           for word in udhr.words(lang + '-Latin1'))\n",
        ">>> cfd.plot(cumulative=True)"
      ],
      "metadata": {
        "id": "xwrkZLgTGh2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = ['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]\n",
        "pairs = [('news', 'The'), ('news', 'Fulton'), ('news', 'County'), ...]"
      ],
      "metadata": {
        "id": "IqBLCnn-GvDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Counting Words by Genre\n",
        ">>> from nltk.corpus import brown\n",
        ">>> cfd = nltk.ConditionalFreqDist(\n",
        "...           (genre, word)\n",
        "...           for genre in brown.categories()\n",
        "...           for word in brown.words(categories=genre))"
      ],
      "metadata": {
        "id": "KugApTfhHCUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ">>> genre_word = [(genre, word)\n",
        "...               for genre in ['news', 'romance']\n",
        "...               for word in brown.words(categories=genre)]\n",
        ">>> len(genre_word)"
      ],
      "metadata": {
        "id": "N3_UswqEHH8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "genre_word[:4]"
      ],
      "metadata": {
        "id": "-Q45yAFRHPd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "genre_word[-4:]"
      ],
      "metadata": {
        "id": "Pl7O5nFVHZti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfd = nltk.ConditionalFreqDist(genre_word)\n",
        "cfd"
      ],
      "metadata": {
        "id": "4ID4jORNHbiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfd.conditions()"
      ],
      "metadata": {
        "id": "3-HrT4TKHgG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfd['news']\n",
        "#<FreqDist with 100554 outcomes>"
      ],
      "metadata": {
        "id": "x5trn_-vHjaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfd['romance']\n",
        "#<FreqDist with 70022 outcomes>"
      ],
      "metadata": {
        "id": "Nqe04fXfHmfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list(cfd['romance'])"
      ],
      "metadata": {
        "id": "na09d2wKHx2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfd['romance']['could']"
      ],
      "metadata": {
        "id": "VY-4V7DvHrLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ">>> from nltk.corpus import inaugural\n",
        ">>> cfd = nltk.ConditionalFreqDist(\n",
        "...           (target, fileid[:4])\n",
        "...           for fileid in inaugural.fileids()\n",
        "...           for w in inaugural.words(fileid)\n",
        "...           for target in ['america', 'citizen']\n",
        "...           if w.lower().startswith(target))"
      ],
      "metadata": {
        "id": "cGOe9WgYHvEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ">>> from nltk.corpus import udhr\n",
        ">>> languages = ['Chickasaw', 'English', 'German_Deutsch',\n",
        "...     'Greenlandic_Inuktikut', 'Hungarian_Magyar', 'Ibibio_Efik']\n",
        ">>> cfd = nltk.ConditionalFreqDist(\n",
        "...           (lang, len(word))\n",
        "...           for lang in languages\n",
        "...           for word in udhr.words(lang + '-Latin1'))"
      ],
      "metadata": {
        "id": "RW6IV8waIAqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ">>> cfd.tabulate(conditions=['English', 'German_Deutsch'],\n",
        "...              samples=range(10), cumulative=True)"
      ],
      "metadata": {
        "id": "1JZEdqwbIEIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_model(cfdist, word, num=15):\n",
        "    for i in range(num):\n",
        "        print(word,)\n",
        "        word = cfdist[word].max()\n",
        "\n",
        "text = nltk.corpus.genesis.words('english-kjv.txt')\n",
        "bigrams = nltk.bigrams(text)\n",
        "cfd = nltk.ConditionalFreqDist(bigrams)"
      ],
      "metadata": {
        "id": "mGcfPPCvIK6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfd['living']"
      ],
      "metadata": {
        "id": "0AKtpC6UIRYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_model(cfd, 'living')"
      ],
      "metadata": {
        "id": "tPaKYIPTIXRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ">>> from __future__ import division\n",
        ">>> def lexical_diversity(text):\n",
        "...     return len(text) / len(set(text))"
      ],
      "metadata": {
        "id": "8TPFm3G6IaXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ">>> def lexical_diversity(my_text_data):\n",
        "...     word_count = len(my_text_data)\n",
        "...     vocab_size = len(set(my_text_data))\n",
        "...     diversity_score = word_count / vocab_size\n",
        "...     return diversity_score"
      ],
      "metadata": {
        "id": "OyQDWTdzIhTc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plural(word):\n",
        "    if word.endswith('y'):\n",
        "        return word[:-1] + 'ies'\n",
        "    elif word[-1] in 'sx' or word[-2:] in ['sh', 'ch']:\n",
        "        return word + 'es'\n",
        "    elif word.endswith('an'):\n",
        "        return word[:-2] + 'en'\n",
        "    else:\n",
        "        return word + 's'"
      ],
      "metadata": {
        "id": "yLiVVpQzIjvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plural('fairy')\n",
        "#plural('woman')"
      ],
      "metadata": {
        "id": "ZjFOf8oKImpr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save your function(s) in a file called (say) textproc.py.\n",
        "# Now, you can access your work simply by importing it from the file\n",
        "\"\"\"\n",
        "from textproc import plural\n",
        "plural('wish')\n",
        "plural('fan')\"\"\""
      ],
      "metadata": {
        "id": "MtzTad6qItYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unusual_words(text):\n",
        "    text_vocab = set(w.lower() for w in text if w.isalpha())\n",
        "    english_vocab = set(w.lower() for w in nltk.corpus.words.words())\n",
        "    unusual = text_vocab.difference(english_vocab)\n",
        "    return sorted(unusual)"
      ],
      "metadata": {
        "id": "0K8cklQvI11u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unusual_words(nltk.corpus.gutenberg.words('austen-sense.txt'))"
      ],
      "metadata": {
        "id": "GufPbZVWJG7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unusual_words(nltk.corpus.nps_chat.words())"
      ],
      "metadata": {
        "id": "HSn1HvnDJJAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "stopwords.words('english')"
      ],
      "metadata": {
        "id": "oLXefaVvJM6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ">>> def content_fraction(text):\n",
        "...     stopwords = nltk.corpus.stopwords.words('english')\n",
        "...     content = [w for w in text if w.lower() not in stopwords]\n",
        "...     return len(content) / len(text)\n",
        "...\n",
        ">>> content_fraction(nltk.corpus.reuters.words())"
      ],
      "metadata": {
        "id": "6D-oZh4PJRMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        " \t\n",
        "puzzle_letters = nltk.FreqDist('egivrvonl')\n",
        "obligatory = 'r'\n",
        "wordlist = nltk.corpus.words.words()\n",
        "[w for w in wordlist if len(w) >= 6\n",
        "                     and obligatory in w\n",
        "                     and nltk.FreqDist(w) <= puzzle_letters]"
      ],
      "metadata": {
        "id": "-P5kwt7zJXFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ">>> names = nltk.corpus.names\n",
        ">>> names.fileids()\n",
        "#['female.txt', 'male.txt']\n",
        ">>> male_names = names.words('male.txt')\n",
        ">>> female_names = names.words('female.txt')\n",
        ">>> [w for w in male_names if w in female_names]"
      ],
      "metadata": {
        "id": "tLCpi51qJgGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ">>> cfd = nltk.ConditionalFreqDist(\n",
        "...           (fileid, name[-1])\n",
        "...           for fileid in names.fileids()\n",
        "...           for name in names.words(fileid))\n",
        ">>> cfd.plot()"
      ],
      "metadata": {
        "id": "JHzraLRHJqF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ">>> entries = nltk.corpus.cmudict.entries()\n",
        ">>> len(entries)\n",
        "#127012\n",
        ">>> for entry in entries[39943:39951]:\n",
        "...     print(entry)"
      ],
      "metadata": {
        "id": "d8aC2sb4Ju2V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ">>> import nltk, re, pprint\n",
        ">>> from nltk import word_tokenize"
      ],
      "metadata": {
        "id": "HT7I6v9-J0mq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Accessing Text from the Web and from Disk"
      ],
      "metadata": {
        "id": "pN54PWUyKT8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#https://www.nltk.org/book/ch03.html\n",
        "#https://www.nltk.org/book/"
      ],
      "metadata": {
        "id": "1hm2kRoDKR47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text Processing steps discussed in this article:\n",
        "Tokenization\n",
        "Lower case conversion\n",
        "Stop Words removal\n",
        "Stemming\n",
        "Lemmatization\n",
        "Parse tree or Syntax Tree generation\n",
        "POS Tagging"
      ],
      "metadata": {
        "id": "x4vbEOyIMGXR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "text = \"Natural language processing is an exciting area.\"\n",
        "\n",
        "print(sent_tokenize(text))"
      ],
      "metadata": {
        "id": "DLcgTO1dL_Bc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_tokenize(text))"
      ],
      "metadata": {
        "id": "xwW_bBWxMLJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower())\n",
        "words = text.split()\n",
        "print(words)"
      ],
      "metadata": {
        "id": "rDF-1NDLMNmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(stopwords.words(\"english\"))"
      ],
      "metadata": {
        "id": "PS-m8PwkMQAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "# Reduce words to their stems\n",
        "stemmed = [PorterStemmer().stem(w) for w in words]\n",
        "print(stemmed)"
      ],
      "metadata": {
        "id": "eTxAb6u7MWdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "# Reduce words to their root form\n",
        "lemmed = [WordNetLemmatizer().lemmatize(w) for w in words]\n",
        "print(lemmed)"
      ],
      "metadata": {
        "id": "4APnzh2xMZfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk import pos_tag, word_tokenize, RegexpParser\n",
        "# Example text\n",
        "sample_text = \"The quick brown fox jumps over the lazy dog\"\n",
        "# Find all parts of speech in above sentence\n",
        "tagged = pos_tag(word_tokenize(sample_text))\n",
        "#Extract all parts of speech from any text\n",
        "chunker = RegexpParser(\"\"\"\n",
        "\t\t\t\t\tNP: {} #To extract Noun Phrases\n",
        "\t\t\t\t\tP: {}\t\t\t #To extract Prepositions\n",
        "\t\t\t\t\tV: {}\t\t\t #To extract Verbs\n",
        "\t\t\t\t\tPP: {} #To extract Prepositional Phrases\n",
        "          VP: {} #To extract Verb Phrases\n",
        "          \"\"\")\n",
        "# Print all parts of speech in above sentence\n",
        "output = chunker.parse(tagged)\n",
        "print(\"After Extracting\", output)"
      ],
      "metadata": {
        "id": "iH6lP4r8Mb5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#output.draw()"
      ],
      "metadata": {
        "id": "TP2GcgkAMosk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "stop_words = set(stopwords.words('english'))\n",
        "txt = \"\"\"Natural language processing is an exciting area.\" \n",
        "       \" Huge budget have been allocated for this.\"\"\"\n",
        "# sent_tokenize is one of instances of\n",
        "# PunktSentenceTokenizer from the nltk.tokenize.punkt module\n",
        "tokenized = sent_tokenize(txt)\n",
        "for i in tokenized:\n",
        "  # Word tokenizers is used to find the words\n",
        "  # and punctuation in a string\n",
        "  wordsList = nltk.word_tokenize(i)\n",
        "  # removing stop words from wordList\n",
        "  wordsList = [w for w in wordsList if not w in stop_words]\n",
        "  # Using a Tagger. Which is part-of-speech\n",
        "  # tagger or POS-tagger.\n",
        "  tagged = nltk.pos_tag(wordsList)\n",
        "  print(tagged)"
      ],
      "metadata": {
        "id": "B5M-8g8cNX-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized = sent_tokenize(txt)\n",
        "for i in tokenized:\n",
        "  # Word tokenizers is used to find the words\n",
        "  # and punctuation in a string\n",
        "  wordsList = nltk.word_tokenize(i)\n",
        "  # removing stop words from wordList\n",
        "  wordsList = [w for w in wordsList if not w in stop_words]\n",
        "  # Using a Tagger. Which is part-of-speech\n",
        "  # tagger or POS-tagger.\n",
        "  tagged = nltk.pos_tag(wordsList)\n",
        "  print(tagged)"
      ],
      "metadata": {
        "id": "qM2uIW00KfLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "kaoD6so-NhY2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}