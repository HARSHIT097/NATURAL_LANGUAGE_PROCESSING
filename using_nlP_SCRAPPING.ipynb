{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "using_nlP_SCRAPPING.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Pgpp8CyEoxhy"
      ],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HARSHIT097/NATURAL_LANGUAGE_PROCESSING/blob/main/using_nlP_SCRAPPING.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pgpp8CyEoxhy"
      },
      "source": [
        "#CHAT-BOT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjBq64PQYruk"
      },
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import random\n",
        "import string # to process standard python strings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiJ0dcugYrun"
      },
      "source": [
        "with open('chatbot_input.txt','r',errors = 'ignore') as f:\n",
        "    raw=f.read()\n",
        "raw=raw.lower()# converts to lowercasenltk.download('punkt') # first-time use only\n",
        "# nltk.download('wordnet') # first-time use only\n",
        "sent_tokens = nltk.sent_tokenize(raw)# converts to list of sentences \n",
        "word_tokens = nltk.word_tokenize(raw)# converts to list of words\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AaGVQM8EYruo"
      },
      "source": [
        "lemmer = nltk.stem.WordNetLemmatizer()\n",
        "#WordNet is a semantically-oriented dictionary of English included in NLTK.\n",
        "def LemTokens(tokens):\n",
        "    return [lemmer.lemmatize(token) for token in tokens]\n",
        "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
        "\n",
        "def LemNormalize(text):\n",
        "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5t6o5L4oYrup"
      },
      "source": [
        "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\",)\n",
        "GREETING_RESPONSES = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
        "def greeting(sentence):\n",
        "    for word in sentence.split():\n",
        "        if word.lower() in GREETING_INPUTS:\n",
        "            return random.choice(GREETING_RESPONSES)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnskJiD7Yrup"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6aXV57HYruq"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KLsxxymYruq"
      },
      "source": [
        "def response(user_response):\n",
        "    robo_response=''\n",
        "    sent_tokens.append(user_response)    \n",
        "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
        "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
        "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
        "    idx=vals.argsort()[0][-2]\n",
        "    flat = vals.flatten()\n",
        "    flat.sort()\n",
        "    req_tfidf = flat[-2]    \n",
        "    if(req_tfidf==0):\n",
        "        robo_response=robo_response+\"I am sorry! I don't understand you\"\n",
        "        return robo_response\n",
        "    else:\n",
        "        robo_response = robo_response+sent_tokens[idx]\n",
        "        return robo_response"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "da3maC2bYrur",
        "outputId": "f349c9e8-9fd8-4198-c604-486ce34cdd24"
      },
      "source": [
        "flag=True\n",
        "print(\"ROBO: My name is Robo. I will answer your queries about Chatbots. If you want to exit, type Bye!\")\n",
        "while(flag==True):\n",
        "    user_response = input()\n",
        "    user_response=user_response.lower()\n",
        "    if(user_response!='bye'):\n",
        "        if(user_response=='thanks' or user_response=='thank you' ):\n",
        "            flag=False\n",
        "            print(\"ROBO: You are welcome..\")\n",
        "        else:\n",
        "            if(greeting(user_response)!=None):\n",
        "                print(\"ROBO: \"+greeting(user_response))\n",
        "            else:\n",
        "                print(\"ROBO: \",end=\"\")\n",
        "                print(response(user_response))\n",
        "                sent_tokens.remove(user_response)\n",
        "    else:\n",
        "        flag=False\n",
        "        print(\"ROBO: Bye! take care..\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROBO: My name is Robo. I will answer your queries about Chatbots. If you want to exit, type Bye!\n",
            "hi\n",
            "ROBO: hello\n",
            "hi\n",
            "ROBO: hi there\n",
            "chatbot and jobs\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "D:\\Installed_new\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "ROBO: [75]\n",
            "\n",
            "chatbots and jobs\n",
            "\n",
            "chatbots are increasingly present in businesses and often are used to automate tasks that do not require skill-based talents.\n",
            "bye\n",
            "ROBO: Bye! take care..\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOeLxAPjLoGV"
      },
      "source": [
        "# extracting articles key words\n",
        "#credits Khuyen Tran\n",
        "#tags - python scrapping nlp"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prw5nr-BMYTp"
      },
      "source": [
        "#!pip install newspaper3k"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QX4h1BrvYruu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09234fb2-0df0-4b07-9f8a-f2a9b6b41774"
      },
      "source": [
        "from newspaper import Article\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2MqncJ0Yruu"
      },
      "source": [
        "url = \"https://www.dataquest.io/blog/learn-data-science/\"\n",
        "article = Article(url)\n",
        "#doenload html\n",
        "article.download()\n",
        "#getting info\n",
        "article.parse()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pe_ZYHUpYruv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c63c7f2c-56e1-4c53-d94c-992dc46a7752"
      },
      "source": [
        "article.title"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'How to Learn Data Science (Step-By-Step) in 2020 – Dataquest'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5wyWpG9Yruv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "024c805e-72bf-4365-cf05-ea1b268628aa"
      },
      "source": [
        "article.publish_date"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "datetime.datetime(2020, 5, 4, 7, 1, tzinfo=tzlocal())"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "IGSvoSWuNYjN",
        "outputId": "b864daaa-0159-46f4-fa09-b0beea1be87a"
      },
      "source": [
        "article.top_image"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'https://www.dataquest.io/wp-content/uploads/2020/05/learn-data-science.jpg'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "e_SLNksPNe-w",
        "outputId": "6510c4d8-b646-4e30-eb6b-349d8bd6320b"
      },
      "source": [
        "article.nlp()\n",
        "article.summary"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'How to Learn Data ScienceSo how do you start to learn data science?\\nIf you want to learn data science or just pick up some data science skills, your first goal should be to learn to love data.\\nRather, consider it as a rough set of guidelines to follow as you learn data science on your own path.\\nI personally believe that anyone can learn data science if they approach it with the right frame of mind.\\nI’m also the founder of Dataquest, a site that helps you learn data science in your browser.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7w_tvzozNl_3",
        "outputId": "4c573b81-4ee4-4e10-bd6f-9495a0eb884e"
      },
      "source": [
        "article.keywords"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['need',\n",
              " 'learn',\n",
              " '2020',\n",
              " 'skills',\n",
              " 'scientists',\n",
              " 'dataquest',\n",
              " 'science',\n",
              " 'youre',\n",
              " 'data',\n",
              " 'youll',\n",
              " 'work',\n",
              " 'stepbystep',\n",
              " 'learning']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAtI-yLxo5nN"
      },
      "source": [
        "# TITLE GENERATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47ZfB1duo9tS",
        "outputId": "284b0537-d8da-4b26-fd3a-b8b71ea4c6bc"
      },
      "source": [
        "import urllib.request #Handling URL\n",
        "\n",
        "from bs4 import BeautifulSoup #Handling or parsing html files\n",
        "\n",
        "import nltk #toolkit\n",
        "nltk.download('stopwords') #or is was extracted\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKntz8WdwrMc"
      },
      "source": [
        "#get the info from website\n",
        "response =  urllib.request.urlopen('https://en.wikipedia.org/wiki/sperm')\n",
        "html = response.read()\n",
        "\n",
        "soup = BeautifulSoup(html,'html5lib')\n",
        "text = soup.get_text(strip = True)\n",
        "\n",
        "tokens = [t for t in text.split()]\n",
        "\n",
        "\n",
        "sr= stopwords.words('english')\n",
        "clean_tokens = tokens[:]\n",
        "for token in tokens:\n",
        "    if token in stopwords.words('english'):\n",
        "        \n",
        "        clean_tokens.remove(token)\n",
        "\n",
        "freq = nltk.FreqDist(clean_tokens)\n",
        "for key,val in freq.items():\n",
        "    print(str(key) + ':' + str(val))\n",
        "freq.plot(20, cumulative=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qhhwi3RxSze"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfMYQp6ewx-O"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUV456bsxRsx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}