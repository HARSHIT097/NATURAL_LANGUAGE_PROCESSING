{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Nlp_section_material_prep.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "6BIxuF6lo7HL"
      ],
      "authorship_tag": "ABX9TyMujy/VSGaaS3vsz6PYY/4f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HARSHIT097/NATURAL_LANGUAGE_PROCESSING/blob/main/Nlp_section_material_prep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sebCNwlDZM64"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BIxuF6lo7HL"
      },
      "source": [
        "# Experimenting with Different Models Nlp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KQtDUeUZT4i"
      },
      "source": [
        "Steps For Natural Language Processing\n",
        "\n",
        "1. Import libraries and Dataset\n",
        "2. Text Cleaning or Preprocessing (Removing Stopwords/punctuations/numbers, stemming, converting to lowercase)\n",
        "3. Tokenization, involves spltting sentences and words from the body of text\n",
        "4. Making the bag of words via sparse matrix\n",
        "    Taking all different words from reviews, removing duplicates\n",
        "5. Splitting corpus into training and test set\n",
        "6. Fitting a predictive model\n",
        "7. Predicting Results (using predict() function)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rX_e2FnlbY8u"
      },
      "source": [
        "#importing libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import sklearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "chcjyf7wbynW",
        "outputId": "de2cb938-819e-4f07-9e8b-c9f3fc001295"
      },
      "source": [
        "#importing Dataset\n",
        "dataset = pd.read_csv(\"/content/Restaurant_Reviews.tsv\", delimiter='\\t', quoting=3)\n",
        "dataset.head(5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Review</th>\n",
              "      <th>Liked</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Wow... Loved this place.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Crust is not good.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Not tasty and the texture was just nasty.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Stopped by during the late May bank holiday of...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The selection on the menu was great and so wer...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              Review  Liked\n",
              "0                           Wow... Loved this place.      1\n",
              "1                                 Crust is not good.      0\n",
              "2          Not tasty and the texture was just nasty.      0\n",
              "3  Stopped by during the late May bank holiday of...      1\n",
              "4  The selection on the menu was great and so wer...      1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nly1lq4KbX4E"
      },
      "source": [
        "'''import pandas\n",
        "df = pandas.read_csv('hrdata.csv', \n",
        "            index_col='Employee', \n",
        "            parse_dates=['Hired'], \n",
        "            header=0, \n",
        "            names=['Employee', 'Hired','Salary', 'Sick Days'])\n",
        "print(df)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0vZuDlQe4Ll"
      },
      "source": [
        "Preprocessing Dataset\n",
        "Each review undergoes through a preprocessing step, where all the vague information is removed.\n",
        "\n",
        "Removing the Stopwords, numeric and speacial charecters.\n",
        "Normalizing each review using the approach of stemming."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5b9eTF-GenjG"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "#nltk.download('stopwords')\n",
        "#nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "#initilising empty array to append clean text\n",
        "corpus = []\n",
        "for i in range(0, 1000):\n",
        "  review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i])\n",
        "  ##\n",
        "  review = re.sub(\"\\s+\", \" \", review)\n",
        "  review = ''.join([i for i in review if not i.isdigit()])\n",
        "  bad_char = [\";\",\":\",\"!\",\"*\",\"-\",\",\",\"--\",\"'\"]\n",
        "  review = ''.join(i for i in review if not i in bad_char)\n",
        "\n",
        "  sentence = nltk.tag.pos_tag(review.split())\n",
        "  edited_sentence = [word for word, tag in sentence if tag !='NNP' and tag !=\"NNPS\" and tag !=\"PRP\" and tag !=\"VBP\" ]\n",
        "  edited_sentence = \" \". join(edited_sentence)\n",
        "\n",
        "#converting all case to lowercases\n",
        "  review = review.lower()\n",
        "  #split to arrays\n",
        "  review = review.split()\n",
        "\n",
        "  #creating porter stemmer object taking stem of each word\n",
        "  ps = PorterStemmer()\n",
        "  review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
        "\n",
        "  #rejoining all the words\n",
        "  review = ' '.join(review)\n",
        "  corpus.append(review)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TUvXO3RfpmZ"
      },
      "source": [
        "Vectorization¶\n",
        "From the cleaned dataset, potential features are extracted and are converted to numerical format. The vectorization techniques are used to convert textual data to numerical format. Using vectorization, a matrix is created where each column represents a feature and each row represents an individual review."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUIwIe_be_YC"
      },
      "source": [
        "# Creating the Bag of Words model using CountVectorizer\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer(max_features = 1500)\n",
        "X = cv.fit_transform(corpus).toarray()\n",
        "y = dataset.iloc[:, 1].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEK0DAWmfvCI"
      },
      "source": [
        "Training and Classification¶\n",
        "Further the data is splitted into training and testing set using Cross Validation technique. This data is used as input to classification algorithm.\n",
        "\n",
        "Classification Algorithms:\n",
        "\n",
        "Algorithms like Decision tree, Support Vector Machine, Logistic Regression, Naive Bayes were implemented and on comparing the evaluation metrics two of the algorithms gave better predictions than others.\n",
        "\n",
        "Multinomial Naive Bayes\n",
        "Bernoulli Naive Bayes\n",
        "Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5w502uhDfuux"
      },
      "source": [
        "# Splitting the dataset into the Training set and Test set\n",
        "#from sklearn.cross_validation import train_test_split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDJkn5OGi1zl",
        "outputId": "c7dfe226-9acf-4f64-baec-518e893db0af"
      },
      "source": [
        "\n",
        "# Fitting Naive Bayes to the Training set\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "classifier = GaussianNB()\n",
        "classifier.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GaussianNB(priors=None, var_smoothing=1e-09)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_fqDxirjx0d"
      },
      "source": [
        "#save model\n",
        "import pickle\n",
        "saved_model = pickle.dumps(classifier)\n",
        "classifier_from_pickle = pickle.loads(saved_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgct7L-Hi8EV"
      },
      "source": [
        "# Predicting the Test set results\n",
        "y_pred = classifier.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOKy1vPmjA6-",
        "outputId": "c1ff82ec-5813-443e-be88-1deb16f96898"
      },
      "source": [
        "# Making the Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print (\"Confusion Matrix:\\n\",cm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix:\n",
            " [[ 82  70]\n",
            " [ 23 125]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwhVOXprjDh9",
        "outputId": "82403b9b-677e-4d6f-9f42-66c00eadd4f0"
      },
      "source": [
        "#classification Report\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test, y_pred, labels=[0,1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.54      0.64       152\n",
            "           1       0.64      0.84      0.73       148\n",
            "\n",
            "    accuracy                           0.69       300\n",
            "   macro avg       0.71      0.69      0.68       300\n",
            "weighted avg       0.71      0.69      0.68       300\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQjAxYQuf6Ql",
        "outputId": "6f64fbaa-b63e-4dc2-fa9d-76344f3f4abc"
      },
      "source": [
        "# Multinomial NB\n",
        "\n",
        "# Fitting Naive Bayes to the Training set\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "classifier = MultinomialNB(alpha=0.1)\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predicting the Test set results\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Making the Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print (\"Confusion Matrix:\\n\",cm)\n",
        "\n",
        "# Accuracy, Precision and Recall\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "score1 = accuracy_score(y_test,y_pred)\n",
        "score2 = precision_score(y_test,y_pred)\n",
        "score3= recall_score(y_test,y_pred)\n",
        "print(\"\\n\")\n",
        "print(\"Accuracy is \",round(score1*100,2),\"%\")\n",
        "print(\"Precision is \",round(score2,2))\n",
        "print(\"Recall is \",round(score3,2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix:\n",
            " [[119  33]\n",
            " [ 34 114]]\n",
            "\n",
            "\n",
            "Accuracy is  77.67 %\n",
            "Precision is  0.78\n",
            "Recall is  0.77\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbXSlOz2go41",
        "outputId": "e798b251-7857-453f-d945-e21ef01b2497"
      },
      "source": [
        "# Bernoulli NB\n",
        "\n",
        "# Fitting Naive Bayes to the Training set\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "classifier = BernoulliNB(alpha=0.8)\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predicting the Test set results\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Making the Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print (\"Confusion Matrix:\\n\",cm)\n",
        "\n",
        "# Accuracy, Precision and Recall\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "score1 = accuracy_score(y_test,y_pred)\n",
        "score2 = precision_score(y_test,y_pred)\n",
        "score3= recall_score(y_test,y_pred)\n",
        "print(\"\\n\")\n",
        "print(\"Accuracy is \",round(score1*100,2),\"%\")\n",
        "print(\"Precision is \",round(score2,2))\n",
        "print(\"Recall is \",round(score3,2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix:\n",
            " [[115  37]\n",
            " [ 32 116]]\n",
            "\n",
            "\n",
            "Accuracy is  77.0 %\n",
            "Precision is  0.76\n",
            "Recall is  0.78\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWVgAS0zgs6E",
        "outputId": "e20a85bb-c95c-4a35-faf3-5d9dcedcd783"
      },
      "source": [
        "# Logistic Regression\n",
        "\n",
        "# Fitting Logistic Regression to the Training set\n",
        "from sklearn import linear_model\n",
        "classifier = linear_model.LogisticRegression(C=1.5)\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predicting the Test set results\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Making the Confusion Matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print (\"Confusion Matrix:\\n\",cm)\n",
        "\n",
        "# Accuracy, Precision and Recall\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "score1 = accuracy_score(y_test,y_pred)\n",
        "score2 = precision_score(y_test,y_pred)\n",
        "score3= recall_score(y_test,y_pred)\n",
        "print(\"\\n\")\n",
        "print(\"Accuracy is \",round(score1*100,2),\"%\")\n",
        "print(\"Precision is \",round(score2,2))\n",
        "print(\"Recall is \",round(score3,2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix:\n",
            " [[125  27]\n",
            " [ 43 105]]\n",
            "\n",
            "\n",
            "Accuracy is  76.67 %\n",
            "Precision is  0.8\n",
            "Recall is  0.71\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9_tWlOsf6AD"
      },
      "source": [
        "Analysis and Conclusion\n",
        "In this study, an attempt has been made to classify sentiment analysis for restaurant reviews using machine learning techniques. Two algorithms namely Multinomial Naive Bayes and Bernoulli Naive Bayes are implemented.\n",
        "\n",
        "Evaluation metrics used here are accuracy, precision and recall.\n",
        "\n",
        "Using Multinomial Naive Bayes,\n",
        "\n",
        "Accuracy of prediction is 77.67%.\n",
        "Precision of prediction is 0.78.\n",
        "Recall of prediction is 0.77.\n",
        "Using Bernoulli Naive Bayes,\n",
        "\n",
        "Accuracy of prediction is 77.0%.\n",
        "Precision of prediction is 0.76.\n",
        "Recall of prediction is 0.78.\n",
        "Using Logistic Regression,\n",
        "\n",
        "Accuracy of prediction is 76.67%.\n",
        "Precision of prediction is 0.8.\n",
        "Recall of prediction is 0.71.\n",
        "From the above results, Multinomial Naive Bayes is slightly better method compared to Bernoulli Naive Bayes and Logistic Regression, with 77.67% accuracy which means the model built for the prediction of sentiment of the restaurant review gives 77.67% right prediction."
      ]
    }
  ]
}