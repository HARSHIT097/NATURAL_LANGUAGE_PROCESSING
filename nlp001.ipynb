{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp001.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMOUVF1y3NlIqRZioJPLI/s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HARSHIT097/NATURAL_LANGUAGE_PROCESSING/blob/main/nlp001.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XICYBcM0coK3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69b12ee3-7663-4f04-cb89-f71ed1769b08"
      },
      "source": [
        "# Importing necessary library\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import os\n",
        "import nltk.corpus\n",
        "# sample text for performing tokenization\n",
        "text = ('In Brazil they drive on the right-hand side of the road. Brazil has a large coastline on the eastern side of South America')\n",
        "# importing word_tokenize from nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "# Passing the string text into word tokenize for breaking the sentences\n",
        "token = word_tokenize(text)\n",
        "token"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['In',\n",
              " 'Brazil',\n",
              " 'they',\n",
              " 'drive',\n",
              " 'on',\n",
              " 'the',\n",
              " 'right-hand',\n",
              " 'side',\n",
              " 'of',\n",
              " 'the',\n",
              " 'road',\n",
              " '.',\n",
              " 'Brazil',\n",
              " 'has',\n",
              " 'a',\n",
              " 'large',\n",
              " 'coastline',\n",
              " 'on',\n",
              " 'the',\n",
              " 'eastern',\n",
              " 'side',\n",
              " 'of',\n",
              " 'South',\n",
              " 'America']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3C6E_yamV2L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8321144-02b4-474f-9559-dc66119371cc"
      },
      "source": [
        "# finding the frequency distinct in the tokens\n",
        "# Importing FreqDist library from nltk and passing token into FreqDist\n",
        "from nltk.probability import FreqDist\n",
        "fdist = FreqDist(token)\n",
        "fdist"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({'.': 1,\n",
              "          'America': 1,\n",
              "          'Brazil': 2,\n",
              "          'In': 1,\n",
              "          'South': 1,\n",
              "          'a': 1,\n",
              "          'coastline': 1,\n",
              "          'drive': 1,\n",
              "          'eastern': 1,\n",
              "          'has': 1,\n",
              "          'large': 1,\n",
              "          'of': 2,\n",
              "          'on': 2,\n",
              "          'right-hand': 1,\n",
              "          'road': 1,\n",
              "          'side': 2,\n",
              "          'the': 3,\n",
              "          'they': 1})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEsTrZ-CnJm3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ee09496-f3bc-42c1-c86c-10f350379872"
      },
      "source": [
        "# To find the frequency of top 10 words\n",
        "fdist1 = fdist.most_common(10)\n",
        "fdist1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 3),\n",
              " ('Brazil', 2),\n",
              " ('on', 2),\n",
              " ('side', 2),\n",
              " ('of', 2),\n",
              " ('In', 1),\n",
              " ('they', 1),\n",
              " ('drive', 1),\n",
              " ('right-hand', 1),\n",
              " ('road', 1)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmXz6s8MnU7E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f19249d3-3926-4a8d-9038-31100281be68"
      },
      "source": [
        "# Importing Porterstemmer from nltk library\n",
        "# Checking for the word ‘giving’ \n",
        "from nltk.stem import PorterStemmer\n",
        "pst = PorterStemmer()\n",
        "pst.stem('waiting')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'wait'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvPcA4i9uljL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d734579a-629e-4439-a44a-795d5381f3d7"
      },
      "source": [
        "# Checking for the list of words\n",
        "stm = [\"waited\", \"waiting\", \"waits\"]\n",
        "for word in stm :\n",
        "   print(word+ \":\" +pst.stem(word))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "waited:wait\n",
            "waiting:wait\n",
            "waits:wait\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SATx_xK7vCv0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d4312be-1a6c-4a7e-ed46-664a6cab9291"
      },
      "source": [
        "# Importing LancasterStemmer from nltk\n",
        "from nltk.stem import LancasterStemmer\n",
        "lst = LancasterStemmer()\n",
        "stm = [\"giving\", \"given\", \"given\", \"gave\"]\n",
        "for word in stm :\n",
        " print(word+ \":\" +lst.stem(word))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "giving:giv\n",
            "given:giv\n",
            "given:giv\n",
            "gave:gav\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5x9oeb79vKeX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8725f2d8-d9f4-4483-ba25-99e6f327330b"
      },
      "source": [
        "# Importing Lemmatizer library from nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer() \n",
        " \n",
        "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\")) \n",
        "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "rocks : rock\n",
            "corpora : corpus\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "defflTs6vvp4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62bec0bd-3e1a-44b6-c6d1-5ec59b573d23"
      },
      "source": [
        "# importing stopwors from nltk library\n",
        "from nltk import word_tokenize\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "a = set(stopwords.words('english'))\n",
        "text = 'Cristiano Ronaldo was born on February 5, 1985, in Funchal, Madeira, Portugal.'\n",
        "text1 = word_tokenize(text.lower())\n",
        "print(text1)\n",
        "stopwords = [x for x in text1 if x not in a]\n",
        "print(stopwords)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "['cristiano', 'ronaldo', 'was', 'born', 'on', 'february', '5', ',', '1985', ',', 'in', 'funchal', ',', 'madeira', ',', 'portugal', '.']\n",
            "['cristiano', 'ronaldo', 'born', 'february', '5', ',', '1985', ',', 'funchal', ',', 'madeira', ',', 'portugal', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYMUqoDkwD9D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c200cc01-0b0e-4db5-cfaf-d0613cbcbb29"
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "text = \"vote to choose a particular man or a group (party) to represent them in parliament\"\n",
        "#Tokenize the text\n",
        "tex = word_tokenize(text)\n",
        "for token in tex:\n",
        "  print(nltk.pos_tag([token]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[('vote', 'NN')]\n",
            "[('to', 'TO')]\n",
            "[('choose', 'NN')]\n",
            "[('a', 'DT')]\n",
            "[('particular', 'JJ')]\n",
            "[('man', 'NN')]\n",
            "[('or', 'CC')]\n",
            "[('a', 'DT')]\n",
            "[('group', 'NN')]\n",
            "[('(', '(')]\n",
            "[('party', 'NN')]\n",
            "[(')', ')')]\n",
            "[('to', 'TO')]\n",
            "[('represent', 'NN')]\n",
            "[('them', 'PRP')]\n",
            "[('in', 'IN')]\n",
            "[('parliament', 'NN')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDVm9FqmwWDm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ea7774b-541b-4aef-af24-864990e1839c"
      },
      "source": [
        "#nltk.download('maxent_ne_chunker')\n",
        "#nltk.download('words')\n",
        "text = \"Google’s CEO Sundar Pichai introduced the new Pixel at Minnesota Roi Centre Event\"\n",
        "#importing chunk library from nltk\n",
        "from nltk import ne_chunk\n",
        "# tokenize and POS Tagging before doing chunk\n",
        "token = word_tokenize(text)\n",
        "tags = nltk.pos_tag(token)\n",
        "chunk = ne_chunk(tags)\n",
        "print(chunk)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(S\n",
            "  (PERSON Google/NNP)\n",
            "  ’/NNP\n",
            "  s/VBD\n",
            "  (ORGANIZATION CEO/NNP Sundar/NNP Pichai/NNP)\n",
            "  introduced/VBD\n",
            "  the/DT\n",
            "  new/JJ\n",
            "  Pixel/NNP\n",
            "  at/IN\n",
            "  (ORGANIZATION Minnesota/NNP Roi/NNP Centre/NNP)\n",
            "  Event/NNP)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5f2vBJIxo6R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c47dd70d-e996-4aff-950f-02af6cb3dd53"
      },
      "source": [
        "text = 'We saw the yellow dog'\n",
        "token = word_tokenize(text)\n",
        "tags = nltk.pos_tag(token)\n",
        "reg = \"NP: {<DT>?<JJ>*<NN>}\"\n",
        "a = nltk.RegexpParser(reg)\n",
        "result = a.parse(tags)\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(S We/PRP saw/VBD (NP the/DT yellow/JJ dog/NN))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jd9rb3dcy8FK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa5026b5-000f-4e61-b916-91bf9161f946"
      },
      "source": [
        "# pip install spacy\n",
        "# python -m spacy download en_core_web_sm\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Load English tokenizer, tagger, parser, NER and word vectors\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Process whole documents\n",
        "text = (\"When Sebastian Thrun started working on self-driving cars at \"\n",
        "        \"Google in 2007, few people outside of the company took him \"\n",
        "        \"seriously. “I can tell you very senior CEOs of major American \"\n",
        "        \"car companies would shake my hand and turn away because I wasn’t \"\n",
        "        \"worth talking to,” said Thrun, in an interview with Recode earlier \"\n",
        "        \"this week.\")\n",
        "doc = nlp(text)\n",
        "\n",
        "# Analyze syntax\n",
        "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
        "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
        "\n",
        "# Find named entities, phrases and concepts\n",
        "for entity in doc.ents:\n",
        "    print(entity.text, entity.label_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Noun phrases: ['Sebastian Thrun', 'self-driving cars', 'Google', 'few people', 'the company', 'him', 'I', 'you', 'very senior CEOs', 'major American car companies', 'my hand', 'I', 'Thrun', 'an interview', 'Recode']\n",
            "Verbs: ['start', 'work', 'drive', 'take', 'can', 'tell', 'would', 'shake', 'turn', 'talk', 'say']\n",
            "Sebastian NORP\n",
            "Google ORG\n",
            "2007 DATE\n",
            "American NORP\n",
            "Recode ORG\n",
            "earlier this week DATE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FevhCYqr0LXX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}